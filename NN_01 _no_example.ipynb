{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 OR 1 NUERAL NETWORK DETECTOR ~95% accuracy, although may vary from low to high 90's for accuracy\n",
    "\n",
    "#BY: Rahul.Gudise\n",
    "# https://github.com/Chipsandnonos\n",
    "\n",
    "#Trained using the MNIST database, acessed here http://yann.lecun.com/exdb/mnist/\n",
    "#Developed using NUMPY\n",
    "\n",
    "#Feel free to fork, and send in suggestions for improvements\n",
    "\n",
    "import numpy\n",
    "from mnist import MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = MNIST('samples')\n",
    "images_train, labels_train = ndata.load_training()\n",
    "images_test, labels_test = ndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The \"squish\" function choosen for this particular Nueral Network is the sigmoid (logistic) curve\n",
    "def sigmoid(x): \n",
    "    return 1/(1+numpy.exp(-x))\n",
    "\n",
    "#This is the derivative of the sigmoid curve\n",
    "def sigmoid_p(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NETWORK STRUCTURE\n",
    "# The goal of this Nueral Network is to detect whether a 28x28 grayscale image is that of a 1 or 0\n",
    "\n",
    "#Due to there only being 2 answers, a compact Neural Network structure was picked\n",
    "#Layer 1 consists of 784 nuerons, each corresponding to a pixel within the original 28x28 image\n",
    "#Layer 2 consists of 5 nuerons\n",
    "#Layer 3 consists of 1 nueron, which when it has an activation of 1, signifies the input was a 1, and when 0, means the input was 0\n",
    "\n",
    "\n",
    "#ALL GLOBAL VARIABLES\n",
    "pic_size = 28              #The size of the original grayscale picture                \n",
    "lay1_size = pic_size**2    #Size of the first layer (28x28)\n",
    "lay2_size = 5           #Size of the second layer (5)\n",
    "lay3_size = 1              #Size of the third layer (1))\n",
    "\n",
    "#HYPERPARAMTER \n",
    "learn_rate = .01   #The learning rate of the nueral network, a small value was chosen so that the change vector would not\n",
    "                     #overshoot when adjusting the weights and biases for the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will Pick out only 1's and 0's in MNIST TRAINING DATASET\n",
    "num_shift = 0\n",
    "for i in range(len(labels_train)):\n",
    "    \n",
    "    if not (labels_train[i-num_shift] == 0 or labels_train[i-num_shift] == 1) :\n",
    "        images_train.pop(i-num_shift)\n",
    "        labels_train.pop(i-num_shift)\n",
    "        num_shift = num_shift +1\n",
    "   \n",
    "        \n",
    "\n",
    "#Will Pick out only 1's and 0's in MNIST TESTING DATASET\n",
    "num_shift = 0\n",
    "\n",
    "for i in range(len(labels_test)):\n",
    "    \n",
    "    if not (labels_test[i-num_shift] == 0 or labels_test[i-num_shift] == 1) :\n",
    "        labels_test.pop(i-num_shift)\n",
    "        images_test.pop(i-num_shift)\n",
    "        num_shift = num_shift +1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function which will take all grayscale pixel values in an original MNIST data piece and transfer into the activations\n",
    "#of the first layer nuerons\n",
    "#The value \"scale\", is there simply to convert the grayscale value (0-255), into a number which can be inputted into the nueron\n",
    "#Nueron domain is 0-1\n",
    "\n",
    "def img_to_activation(layer1A, image):\n",
    "    scale = 1/255\n",
    "    \n",
    "    for i in range(lay1_size):\n",
    "        layer1A[i] = image[i]*scale\n",
    "    \n",
    "    return layer1A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function which will take in the activations of 1st layer nuerons, and with the weights and biases of all layers, to \n",
    "#computer the appropriate activations of 2nd and 3rd layer nuerons \n",
    "def find_activation(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32): #need to use the biases too\n",
    "    \n",
    "    \n",
    "    layer2A = numpy.matmul(weight21, layer1A)\n",
    "    \n",
    "    for i in range(lay2_size):\n",
    "        layer2A[i] = layer2A[i] + bias21[i]\n",
    "    \n",
    "    for i in range(lay2_size):\n",
    "        layer2A[i] = sigmoid(layer2A[i])\n",
    "    \n",
    "    layer3A = numpy.matmul(weight32, layer2A)\n",
    "    layer3A = layer3A + bias32\n",
    "    layer3A = sigmoid(layer3A)\n",
    "   \n",
    "    return layer2A, layer3A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function which will calculate the change vector which points in the direction of greatest ascent of the cost function\n",
    "#When given the all activations of all nuerons on all layers, and all weights and biases \n",
    "def deriv_fill(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32, actual):\n",
    "    \n",
    "    #Change Matrices\n",
    "    \n",
    "    cweightL1 = numpy.zeros((lay2_size,lay1_size), dtype = float)\n",
    "    cweightL2 = numpy.zeros((lay2_size),dtype = float )\n",
    "    \n",
    "    cbiasL1 = numpy.zeros((lay2_size),dtype = float )\n",
    "    cbiasL2 = numpy.zeros((lay3_size),dtype = float )\n",
    "   \n",
    "    #Constants needed to compute derivatives \n",
    "    pred = layer3A\n",
    "    z = 0\n",
    "    for i in range(lay2_size):\n",
    "        z = layer2A[i] * weight32[i]\n",
    "    z = z + bias32\n",
    "    \n",
    "    num_w_2 = lay2_size \n",
    "    num_w_1 = lay1_size*lay2_size \n",
    "    #--------------------------------------- Derivatives\n",
    "          \n",
    "    dc_dsig = 2*(pred-actual)\n",
    "    dsig_dz = sigmoid_p(z)\n",
    "    \n",
    "    #dz_d(any layer 2 weight) = activation, bias = 1\n",
    "    \n",
    "    for i in range(lay2_size):\n",
    "        dz_dw0i = layer2A[i]\n",
    "        cweightL2[i] = dc_dsig*dsig_dz*dz_dw0i\n",
    "    cbiasL2[0] = dc_dsig*dsig_dz\n",
    "    \n",
    "    for i in range(lay2_size):\n",
    "        cbiasL1[i] = 1\n",
    "        dz_da2i = weight32[i]\n",
    "        for y in range (lay1_size):\n",
    "            da2i_dwiy = layer1A[y]\n",
    "            cweightL1[i][y] =  dc_dsig*dsig_dz*dz_da2i*da2i_dwiy\n",
    "\n",
    "    \n",
    "  \n",
    "    return cweightL1,cweightL2,cbiasL1,cbiasL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function which will update all parameters (weights and biases), based on previously calculated direction vector\n",
    "def update_val(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32, cweightL1,cweightL2,cbiasL1,cbiasL2 ):\n",
    "     #updates weights connecting layers 1-2    \n",
    "    for i in range (lay2_size):\n",
    "        for y in range (lay1_size):\n",
    "            weight21[i][y] = weight21[i][y] - (learn_rate*cweightL1[i][y])\n",
    "    \n",
    "        #updates weights connected layers 2-3\n",
    "    for i in range (lay2_size):\n",
    "        weight32[i] = weight32[i] - (learn_rate*cweightL2[i])\n",
    "    \n",
    "        #updates biases in layer 2 nuerons\n",
    "    for i in range (lay2_size):\n",
    "        bias21[i] = bias21[i] - (learn_rate*bias21[i])\n",
    "    \n",
    "        #updates biases in layer 3 nuerons\n",
    "    bias32[0] = bias32[0] - (learn_rate*cbiasL2[0])\n",
    "    \n",
    "    return weight21, weight32, bias21, bias32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function which will input a TEST data piece and see the nueral network's output\n",
    "def ask(image, weight21, weight32, bias21, bias32,layer1A,layer2A,layer3A):\n",
    "    layer1A = img_to_activation(layer1A, image)\n",
    "    layer2A, layer3A = find_activation(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32)\n",
    "    \n",
    "    return layer3A   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function which handles the output of the network after being asked to identify a data piece \n",
    "def test(index, weight21,weight32,bias21,bias32,layer1A,layer2A,layer3A):\n",
    "    pred = ask(images_test[index], weight21, weight32, bias21, bias32,layer1A,layer2A,layer3A)\n",
    "    actual = labels_test[index]\n",
    "    num = 0\n",
    "    succeed = 0.0\n",
    "    adj = 0\n",
    "    cost = (pred - actual)**2\n",
    "    if (pred>.5):\n",
    "        num = 1\n",
    "        adj = 0\n",
    "    else:\n",
    "        num = 0\n",
    "        adj = 1\n",
    "    \n",
    "    if (num == actual):\n",
    "        succeed = 1.0\n",
    "    else:\n",
    "        succeed = 0.0\n",
    "    \n",
    "    print(f\"Computer: {num}, with a {adj - pred}%\")\n",
    "    print(f\"Actually: {actual}, with a cost of {cost}\")\n",
    "    return succeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function which ties together all of the above training functions, and trains the nueral network\n",
    "def train():\n",
    "       \n",
    "    #To avoid refering to global variables as much, all matrices and vectors were defined here, and transfered between functions as needed\n",
    "    \n",
    "    #Activations -- All Activations of nuerons are held here\n",
    "    layer1A = numpy.zeros((lay1_size),dtype = float )\n",
    "    layer2A = numpy.zeros((lay2_size),dtype = float )\n",
    "    layer3A = numpy.zeros((lay3_size),dtype = float )\n",
    "\n",
    "    #Weights -- All weights are stored here\n",
    "    weight21 = numpy.zeros((lay2_size,lay1_size), dtype = float)\n",
    "    weight32 = numpy.zeros((lay2_size),dtype = float )\n",
    "\n",
    "    #Biases -- All biases are stored here\n",
    "    bias21 = numpy.zeros((lay2_size),dtype = float )\n",
    "    bias32 = numpy.zeros((lay3_size),dtype = float )\n",
    "    \n",
    "    #Weights and biases start of random, as such this will randomize all enteries in the weight and bias matrices \n",
    "    \n",
    "    #Randomizes all biases and weights in layers 1-2\n",
    "    for i in range(lay2_size):\n",
    "        bias21[i] = numpy.random.randn()\n",
    "        for y in range(lay1_size):\n",
    "            weight21[i][y] = numpy.random.randn()\n",
    "\n",
    "    #Randomizes all biases and weights in layers 2-3\n",
    "    for i in range(5):\n",
    "        weight32[i] = numpy.random.randn()\n",
    "\n",
    "    bias32[0] = numpy.random.randn()\n",
    "\n",
    "    #--------------- Training Process\n",
    "           #1. Transfers the pixel grayscale values into Layer 1 nueron activations\n",
    "           #2. Finds the activations of all the other layers\n",
    "           #3. Finds the appropriate direction vector to find direction of greatest ascent in cost function\n",
    "           #4. Updates all bias and weights based on the aforementioned direction vector\n",
    "            \n",
    "    for i in range (len(labels_train)):\n",
    "        layer1A = img_to_activation(layer1A, images_train[i])\n",
    "        layer2A, layer3A = find_activation(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32)\n",
    "        cweightL1,cweightL2,cbiasL1,cbiasL2 = deriv_fill(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32, labels_train[i])\n",
    "        weight21, weight32, bias21, bias32 = update_val(layer1A, layer2A, layer3A, weight21, weight32, bias21, bias32, cweightL1,cweightL2,cbiasL1,cbiasL2 )\n",
    "        \n",
    "        #A small piece of code to output every 100th cost, so that you can monitor the network's progress\n",
    "        cost = (layer3A - labels_train[i])**2\n",
    "        if i%100 == 0:\n",
    "            print (cost)\n",
    "    \n",
    "    #Wipes all layer's nueron's activations\n",
    "    layer1A = numpy.zeros((lay1_size),dtype = float )\n",
    "    layer2A = numpy.zeros((lay2_size),dtype = float )\n",
    "    layer3A = numpy.zeros((lay3_size),dtype = float )\n",
    "    \n",
    "    return weight21,weight32,bias21,bias32,layer1A,layer2A,layer3A\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains the network\n",
    "weight21,weight32,bias21,bias32,layer1A,layer2A,layer3A = train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Small piece of code to run through the MNIST TEST DATA through the trained nueral network\n",
    "\n",
    "accuracy = 0\n",
    "for i in range(len(labels_test)):\n",
    "    counter = test(i, weight21,weight32,bias21,bias32,layer1A,layer2A,layer3A)\n",
    "    accuracy = accuracy + counter\n",
    "\n",
    "p_accuracy = accuracy/len(labels_test)\n",
    "print(f\"Computer finished testing with a final accuracy of: {p_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
